{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OptiReduce","text":""},{"location":"#optireduce","title":"OptiReduce","text":"<p>OptiReduce is a resilient and tail-optimal collective-communication framework for distributed deep learning in the cloud. It seamlessly integrates with PyTorch's Distributed Data Parallel (DDP) training to optimize communication performance in high-tail latency environments.</p>"},{"location":"#why-optireduce","title":"Why OptiReduce?","text":""},{"location":"#faster-training","title":"\ud83d\ude80 Faster Training","text":"<ul> <li>Eliminates bottlenecks from stragglers and slow workers</li> <li>Optimizes performance in high-tail latency environments</li> <li>Reduces training time, cost, and resource usage</li> </ul>"},{"location":"#key-features","title":"\ud83d\udca1 Key Features","text":"<ul> <li>Bounded-Loss Reliability: Speeds up training by tolerating small controlled loss</li> <li>Tail Latency Optimization: Efficiently handles network variability</li> <li>PyTorch Integration: Seamless integration with PyTorch DDP</li> </ul>"},{"location":"#technical-highlights","title":"\ud83d\udd27 Technical Highlights","text":"<ul> <li>Compatible with Mellanox ConnectX NICs</li> <li>Efficient memory pool management</li> <li>Zero-copy operations for maximum performance</li> </ul>"},{"location":"#choose-your-path-with-optireduce","title":"Choose your path with OptiReduce","text":""},{"location":"#quick-start","title":"\ud83d\udc49 Quick Start","text":"<ul> <li>Getting Started</li> <li>Installation Guide</li> <li>Usage Guide</li> </ul>"},{"location":"#learn-more","title":"\ud83d\udcda Learn More","text":"<ul> <li>Technical Details</li> <li>Benchmarking Guide</li> </ul>"},{"location":"#research","title":"Research","text":"<p>OptiReduce was presented at NSDI '25. Read our paper:</p> <p>\ud83d\udcc4 OptiReduce: Resilient and Tail-Optimal AllReduce for Distributed Deep Learning in the Cloud</p>"},{"location":"#support","title":"Support","text":"<p>Need help? Here are your options:</p> <ul> <li>Review all the documentation</li> <li>Open an issue</li> <li>Review installation guide for setup help</li> </ul>"},{"location":"ansible/","title":"Ansible Deployment Guide","text":""},{"location":"ansible/#download","title":"Download","text":"<p>Clone the ansible repository:</p> <pre><code>git clone https://github.com/OptiReduce/ansible.git\ncd ansible\n</code></pre>"},{"location":"ansible/#prerequisites","title":"Prerequisites","text":""},{"location":"ansible/#ansible-installation","title":"Ansible Installation","text":"<pre><code># For Ubuntu/Debian\nsudo apt update\nsudo apt install software-properties-common\nsudo apt-add-repository --yes --update ppa:ansible/ansible\nsudo apt install ansible\n\n# For RHEL/CentOS\nsudo yum install epel-release\nsudo yum install ansible\n\n# Verify installation\nansible --version\n</code></pre>"},{"location":"ansible/#ssh-setup","title":"SSH Setup","text":"<ul> <li>Ensure SSH access to target machines</li> <li>Configure SSH keys for passwordless authentication</li> <li>Test connection to all target machines</li> </ul>"},{"location":"ansible/#directory-structure","title":"Directory Structure","text":"<pre><code>ansible/\n\u251c\u2500\u2500 ansible.cfg\n\u251c\u2500\u2500 inventory\n\u2502   \u2514\u2500\u2500 hosts\n\u251c\u2500\u2500 group_vars\n\u2502   \u2514\u2500\u2500 all.yml\n\u251c\u2500\u2500 optireduce_deploy.yml\n\u251c\u2500\u2500 Makefile\n\u2514\u2500\u2500 roles/\n    \u251c\u2500\u2500 cuda/\n    \u251c\u2500\u2500 mellanox/\n    \u251c\u2500\u2500 anaconda/\n    \u251c\u2500\u2500 optireduce/\n    \u2514\u2500\u2500 benchmark/\n</code></pre>"},{"location":"ansible/#configuration","title":"Configuration","text":""},{"location":"ansible/#inventory-setup","title":"Inventory Setup","text":"<p>Edit <code>inventory/hosts</code> to specify your target machines:</p> <pre><code>[gpu_nodes]\nnode1 ansible_host=192.168.1.101 ansible_user=test ansible_become_password=test\nnode2 ansible_host=192.168.1.102 ansible_user=test ansible_become_password=test\n</code></pre>"},{"location":"ansible/#variable-configuration","title":"Variable Configuration","text":"<p>Edit <code>group_vars/all.yml</code> to customize versions and settings:</p> <pre><code># CUDA settings\ncuda_version: \"11.7.0-1\"\nnvidia_version: \"560\"\ncudnn_version: \"8.5.0.96-1+cuda11.7\"\n\n# Python/Conda settings\npython_version: \"3.9.19\"\ndpdk_version: \"v20.11\"\n\n# Other settings...\n</code></pre>"},{"location":"ansible/#deployment-options","title":"Deployment Options","text":"<p>The deployment can be customized using the provided Makefile:</p>"},{"location":"ansible/#full-installation","title":"Full Installation","text":"<pre><code>make optireduce-full\n</code></pre>"},{"location":"ansible/#selective-installation","title":"Selective Installation","text":"<pre><code># Install only CUDA\nmake cuda-only\n\n# Install only benchmarking tools\nmake benchmark-only\n\n# Custom installation\nmake deploy INSTALL_CUDA=true INSTALL_BENCHMARK=true\n</code></pre>"},{"location":"ansible/#check-configuration","title":"Check Configuration","text":"<pre><code>make check\n</code></pre>"},{"location":"ansible/#available-components","title":"Available Components","text":"<p>You can selectively install the following components:</p> <ul> <li>CUDA (11.7) and cuDNN (8.5)</li> <li>Mellanox OFED</li> <li>Anaconda with Python 3.9.19</li> <li>DPDK v20.11</li> <li>OptiReduce core</li> <li>Benchmarking tools</li> </ul>"},{"location":"ansible/#environment-variables","title":"Environment Variables","text":"<p>The following environment variables can be set to customize the deployment:</p> <pre><code>INSTALL_CUDA=true/false\nINSTALL_MELLANOX=true/false\nINSTALL_ANACONDA=true/false\nINSTALL_OPTIREDUCE=true/false\nINSTALL_BENCHMARK=true/false\n</code></pre>"},{"location":"ansible/#common-issues-and-troubleshooting","title":"Common Issues and Troubleshooting","text":""},{"location":"ansible/#ssh-connection-issues","title":"SSH Connection Issues","text":"<ul> <li>Verify SSH keys are properly set up</li> <li>Check network connectivity</li> <li>Ensure proper permissions on SSH keys</li> </ul>"},{"location":"ansible/#cuda-installation-failures","title":"CUDA Installation Failures","text":"<ul> <li>Verify system compatibility</li> <li>Check for sufficient disk space</li> <li>Ensure proper network connectivity to NVIDIA repositories</li> </ul>"},{"location":"ansible/#ofed-installation-issues","title":"OFED Installation Issues","text":"<ul> <li>Verify kernel compatibility</li> <li>Check system prerequisites</li> <li>Ensure proper network connectivity</li> </ul>"},{"location":"ansible/#additional-resources","title":"Additional Resources","text":"<ul> <li>OptiReduce Documentation</li> <li>DPDK Documentation</li> <li>CUDA Documentation</li> <li>Mellanox OFED Documentation</li> </ul>"},{"location":"benchmarks/","title":"OptiReduce Benchmarking Guide","text":"<p>This guide explains how to run benchmarks with OptiReduce under different network conditions using controlled background traffic to simulate various network environments.</p> <p>Note</p> <p>If you have already set up the environment using <code>make optireduce-full</code> from the ansible repository, you can skip directly to the Running Training section.</p>"},{"location":"benchmarks/#installation-options","title":"Installation Options","text":""},{"location":"benchmarks/#option-1-using-ansible-recommended","title":"Option 1: Using Ansible (Recommended)","text":"<p>The easiest way to install the benchmark is using our Ansible playbooks:</p> <pre><code>git clone https://github.com/OptiReduce/ansible.git\ncd ansible\nmake benchmark-only\n</code></pre> <p>For detailed instructions on using the Ansible deployment, visit our Ansible documentation.</p>"},{"location":"benchmarks/#option-2-using-benchmark-repository","title":"Option 2: Using Benchmark Repository","text":"<p>We provide automated install scripts in our benchmark repository:</p> <pre><code># Clone the benchmark repository\ngit clone https://github.com/OptiReduce/benchmark.git\ncd benchmark\n\n# Install benchmark\nmake install\n</code></pre>"},{"location":"benchmarks/#option-3-manual-installation","title":"Option 3: Manual Installation","text":"<p>If you prefer to install manually, follow these steps:</p> <ol> <li> <p>Install Redis server: <pre><code>sudo apt update\nsudo apt install redis-server\n</code></pre></p> </li> <li> <p>Clone and build Gloo benchmark: <pre><code># Clone specific version of Gloo\ngit clone https://github.com/facebookincubator/gloo.git\ncd gloo\ngit checkout e6d509b527712a143996f2f59a10480efa804f8b\n\n# Create build directory\nmkdir build\ncd build\n\n# Configure and build\ncmake ../ -DUSE_REDIS=1 -DBUILD_BENCHMARK=1\nmake -j$(nproc)\n</code></pre></p> </li> </ol>"},{"location":"benchmarks/#background-traffic-setup","title":"Background Traffic Setup","text":""},{"location":"benchmarks/#start-redis-server","title":"Start Redis Server","text":"<p>Choose any ONE node to run the Redis server:</p> <pre><code># Start Redis server\nredis-server --port 6199 --protected-mode no\n\n# Clear Redis entries (run before each benchmark)\nredis-cli -p 6199 FLUSHALL\n</code></pre> <p>Important</p> <p>Always clear Redis entries before starting a new benchmark run.</p>"},{"location":"benchmarks/#create-network-environment","title":"Create Network Environment","text":"<p>You can simulate different network environments by varying the number of workers for background traffic script (<code>run_background.sh</code>). This will result in different tail-to-median latency ratios and allows you to simulate different network conditions by adjusting the <code>SIZE</code> parameter:</p> <pre><code>Usage: ./run_background.sh -s SIZE -r STARTING_RANK -t TIME [-H REDIS_HOST] [-p REDIS_PORT] [-d DEVICE]\n\nOptions:\n  -s SIZE           Number of processes\n  -r STARTING_RANK  Starting rank\n  -t TIME           Iteration time in seconds\n  -H REDIS_HOST     Redis host (default: 192.168.100.30)\n  -p REDIS_PORT     Redis port (default: 6199)\n  -d DEVICE         Network device (default: ens17)\n</code></pre>"},{"location":"benchmarks/#example-low-tail-environment-p99p50-15x","title":"Example Low-Tail Environment (p99/p50 = 1.5x)","text":"<p>Run these commands on any two nodes:</p> <pre><code># On first node\n./run_background.sh -s 4 -r 0 -t 240000 -H &lt;redis_host&gt; -d ens17\n\n# On second node\n./run_background.sh -s 4 -r 1 -t 240000 -H &lt;redis_host&gt; -d ens17\n</code></pre>"},{"location":"benchmarks/#example-high-tail-environment-p99p50-3x","title":"Example High-Tail Environment (p99/p50 = 3x)","text":"<p>For the high-tail environment, increase the SIZE parameter:</p> <pre><code># On first node\n./run_background.sh -s 16 -r 0 -t 240000 -H &lt;redis_host&gt; -d ens17\n\n# On second node\n./run_background.sh -s 16 -r 1 -t 240000 -H &lt;redis_host&gt; -d ens17\n</code></pre>"},{"location":"benchmarks/#parameter-explanation","title":"Parameter Explanation","text":"<ul> <li><code>-s SIZE</code>: Number of processes to spawn. Higher values create more background traffic:<ul> <li>4 processes: Creates low-tail environment (p99/p50 \u2248 1.5x)</li> <li>16 processes: Creates high-tail environment (p99/p50 \u2248 3x)</li> </ul> </li> <li><code>-r STARTING_RANK</code>: Starting rank for processes (0 or 1 for two-node setup)</li> <li><code>-t TIME</code>: Duration of background traffic in seconds</li> <li><code>-H REDIS_HOST</code>: Redis server IP address (must be same for all nodes)</li> <li><code>-p REDIS_PORT</code>: Redis server port (default: 6199)</li> <li><code>-d DEVICE</code>: Network interface name (e.g., ens17)</li> </ul> <p>Environment Configuration</p> <p>The size parameters (4 and 16) are based on our test environment. You may need to adjust these values in your environment to achieve similar p99/p50 latency ratios. Monitor your network conditions and adjust accordingly.</p>"},{"location":"benchmarks/#running-training","title":"Running Training","text":"<ol> <li>Create a DPDK configuration file (<code>dpdk.cfg</code>) mapping IP addresses to MAC addresses:</li> </ol> <pre><code>192.168.100.10=AA:BB:CC:DD:EE:FF\n192.168.100.11=AA:BB:CC:DD:EE:00\n</code></pre> <p>Note</p> <p>Ensure all nodes in your cluster are listed in the configuration file.</p> <ol> <li> <p>Clear Redis entries: <pre><code>redis-cli -p 6199 FLUSHALL\n</code></pre></p> </li> <li> <p>Start background traffic for desired environment (low-tail or high-tail) as shown above</p> </li> <li> <p>Run the training script on each node. You have two options:</p> </li> </ol>"},{"location":"benchmarks/#option-1-run-optireduce-only-default","title":"Option 1: Run OptiReduce Only (Default)","text":"<pre><code>./run_training.sh &lt;MASTER_ADDR&gt; &lt;RANK&gt; &lt;NODES&gt; &lt;DEV&gt; &lt;MODEL&gt;\n</code></pre>"},{"location":"benchmarks/#option-2-run-all-communication-schemes","title":"Option 2: Run All Communication Schemes","text":"<pre><code>RUN_ALL=1 ./run_training.sh &lt;MASTER_ADDR&gt; &lt;RANK&gt; &lt;NODES&gt; &lt;DEV&gt; &lt;MODEL&gt;\n</code></pre> <p>This will run the following schemes in order:</p> <ul> <li>NCCL with Ring algorithm</li> <li>NCCL with Tree algorithm</li> <li>Gloo with Ring algorithm</li> <li>Gloo with BCube algorithm</li> <li>Gloo with Transpose algorithm</li> <li>OptiReduce</li> </ul> <p>Available models:</p> <ul> <li>vgg19</li> <li>bert</li> <li>bart</li> <li>roberta</li> <li>gpt2</li> </ul> <p>Example for a 2-node setup with Mellanox NICs: <pre><code># Run only OptiReduce\n# On master node (rank 0)\n./run_training.sh 192.168.1.100 0 2 ens17 bert\n\n# On worker node (rank 1)\n./run_training.sh 192.168.1.100 1 2 ens17 bert\n\n# Run all communication schemes\n# On master node (rank 0)\nRUN_ALL=1 ./run_training.sh 192.168.1.100 0 2 ens17 bert\n\n# On worker node (rank 1)\nRUN_ALL=1 ./run_training.sh 192.168.1.100 1 2 ens17 bert\n</code></pre></p> <p>Parameters:</p> <ul> <li>MASTER_ADDR: IP address of the master node</li> <li>RANK: Node rank (0 for master, 1,2,... for workers)</li> <li>NODES: Total number of nodes</li> <li>DEV: Network device name (e.g., mlx5_0 for Mellanox NICs)</li> <li>MODEL: One of the available models listed above</li> </ul>"},{"location":"benchmarks/#troubleshooting","title":"Troubleshooting","text":""},{"location":"benchmarks/#core-allocation","title":"Core Allocation","text":"<ul> <li>OptiReduce requires at least 4 dedicated CPU cores for running </li> <li>Ensure <code>taskset -c 1-8</code> in run_training.sh matches your system's available cores for PyTorch</li> <li>The <code>--tr_threads_offset</code> parameter should be set to avoid core conflicts with the PyTorch app and must not overlap with the taskset cores</li> <li>Example: If using cores 1-8 for Pytorch, set <code>--tr_threads_offset 11</code> to ensure thread IDs don't overlap</li> </ul>"},{"location":"benchmarks/#timeout-settings","title":"Timeout Settings","text":"<ul> <li>The <code>--tr_timeout</code> parameter is crucial for proper operation</li> <li>Default values in the script:<ul> <li>vgg19: 135</li> <li>bert: 350</li> <li>bart: 370</li> <li>roberta: 370</li> <li>gpt2: 370</li> </ul> </li> <li>You may need to adjust these based on your model size and network conditions</li> <li>For detailed explanation of timeout calculations, refer to our Technical Details page</li> </ul>"},{"location":"benchmarks/#customizing-training-parameters","title":"Customizing Training Parameters","text":"<p>You might need to modify the following parameters in run_training.sh for your specific use case: <pre><code>case $MODEL in\n    vgg19)\n        BATCH_SIZE=128    # Adjust based on your GPU memory\n        EPOCHS=150        # Increase/decrease based on model convergence\n        TR_TIMEOUT=135    # Adjust based on network conditions and number of nodes\n        ;;\n    bert)\n        BATCH_SIZE=16\n        EPOCHS=5\n        TR_TIMEOUT=350\n        ;;\n    # ... other models\nesac\n</code></pre></p>"},{"location":"benchmarks/#results","title":"Results","text":"<p>The following table compares the iteration time (s/it) for different communication strategies, lower is better:</p> Model Env NCCL-Ring NCCL-Tree Ring BCube TAR+TCP OptiReduce GPT-2 1.5 1.70 s 1.52 s 2.20 s 2.45 s 2.12 s 1.39 s 3 2.26 s 1.91 s 2.66 s 2.99 s 2.36 s 1.41 s GPT-2-large 1.5 7.76 s 6.46 s 8.96 s 10.45 s 7.92 s 6.01 s 3 10.12 s 9.34 s 10.60 s 10.80 s 8.48 s 6.07 s BERT-large 1.5 5.01 s 4.24 s 6.10 s 7.30 s 5.90 s 3.76 s 3 6.53 s 5.21 s 8.11 s 8.19 s 6.46 s 3.85 s BART-large 1.5 4.67 s 4.07 s 6.94 s 7.72 s 5.45 s 3.80 s 3 6.90 s 5.74 s 7.70 s 8.11 s 5.88 s 3.90 s RoBERTa-large 1.5 4.75 s 4.15 s 6.12 s 7.64 s 5.94 s 3.87 s 3 7.30 s 5.51 s 8.09 s 8.99 s 6.71 s 3.92 s Llama-3.2 1.5 12.92 s 10.28 s 15.15 s 16.54 s 11.25 s 9.73 s 3 17.28 s 15.72 s 18.84 s 21.97 s 14.59 s 9.98 s"},{"location":"benchmarks/#analysis","title":"Analysis","text":"<ul> <li>OptiReduce consistently outperforms all other methods across different models and environments.</li> <li>Performance gains are especially significant for larger models (GPT-2-large, Llama-3.2), where OptiReduce achieves up to 40% faster iteration time in low-tail environment.</li> <li>The benefits are more pronounced in multi-node environments (<code>Env=3</code>), where communication bottlenecks become more severe and speedups reach around 2x.</li> </ul>"},{"location":"benchmarks/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Performance Degradation</p> <ul> <li>Check CPU core allocation</li> <li>Verify thread offset settings</li> <li>Monitor system for other processes using assigned cores</li> </ul> </li> <li> <p>Training Failures</p> <ul> <li>Ensure adequate timeout values</li> <li>Verify network device names</li> <li>Check Redis server is running and accessible</li> </ul> </li> <li> <p>Network Device Issues</p> <ul> <li>Confirm correct device name (e.g., ens17)</li> <li>Check DPDK binding status</li> <li>Verify hugepages configuration</li> </ul> </li> </ol>"},{"location":"benchmarks/#documentation","title":"Documentation","text":"<ul> <li>Getting Started</li> <li>Technical Details</li> <li>Installation Instructions</li> <li>Usage Instructions</li> </ul>"},{"location":"contributing/","title":"Contributing to OptiReduce","text":"<p>We welcome contributions to OptiReduce! This guide explains how to contribute effectively to the project.</p>"},{"location":"contributing/#contributing-code","title":"Contributing Code","text":""},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li> <p>Create a feature branch for the specific repo: <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make your changes and commit: <pre><code>git commit -m \"Description of changes\"\n</code></pre></p> </li> <li> <p>Update documentation if needed:</p> <ul> <li>New features</li> <li>Configuration options</li> <li>Performance implications</li> </ul> </li> <li> <p>Submit a pull request</p> </li> </ol>"},{"location":"contributing/#pull-request-checklist","title":"Pull Request Checklist","text":"<p>Before submitting your pull request, please ensure:</p> <ul> <li>Documentation is updated</li> <li>Performance impact has been considered</li> <li>Backwards compatibility is maintained</li> <li>Code follows style guidelines</li> <li>All tests pass</li> </ul>"},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":""},{"location":"contributing/#bug-reports","title":"Bug Reports","text":"<p>When reporting a bug, please include:</p> <ul> <li>OptiReduce version (or better yet commit number)</li> <li>System configuration</li> <li>Network environment</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>Relevant logs</li> </ul>"},{"location":"contributing/#feature-requests","title":"Feature Requests","text":"<p>When requesting a feature, please describe:</p> <ul> <li>Use case</li> <li>Expected benefits</li> <li>Performance requirements</li> <li>Resource implications</li> </ul>"},{"location":"contributing/#communication","title":"Communication","text":"<p>We have several channels for communication:</p> <ul> <li>GitHub Issues: Bug reports and feature requests</li> <li>Email: Technical discussions and security reports</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you get started with OptiReduce.</p>"},{"location":"getting-started/#system-requirements","title":"System Requirements","text":""},{"location":"getting-started/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>Network Interface Card (NIC):<ul> <li>Recommended: Mellanox ConnectX NICs (supports flow bifurcation)</li> <li>Alternative: Two NICs (one for TCP-based communication, one DPDK-compatible NIC)</li> </ul> </li> <li>CPU: At least 4 dedicated cores for OptiReduce</li> <li>Memory: 16GB of hugepages</li> </ul>"},{"location":"getting-started/#recommended-software-requirements","title":"Recommended Software Requirements","text":"<ul> <li>Ubuntu 22.04 LTS </li> <li>Python 3.9.19</li> <li>CUDA 11.7 and cuDNN 8.5 (optional, for GPU training)</li> </ul>"},{"location":"getting-started/#installation-options","title":"Installation Options","text":"<p>You can install OptiReduce in two ways:</p>"},{"location":"getting-started/#1-using-ansible-recommended","title":"1. Using Ansible (Recommended)","text":"<p>For automated deployment across multiple nodes:</p> <pre><code>git clone https://github.com/OptiReduce/ansible.git\ncd ansible\nmake optireduce-full\n</code></pre>"},{"location":"getting-started/#2-manual-installation","title":"2. Manual Installation","text":"<p>Clone and install the core repository on all nodes manually:</p> <pre><code>git clone https://github.com/OptiReduce/setup.git\ncd setup\nmake install\n</code></pre> <p>For detailed installation instructions, see our Installation Guide.</p>"},{"location":"getting-started/#quick-start","title":"Quick Start","text":""},{"location":"getting-started/#1-configure-environment","title":"1. Configure Environment","text":"<pre><code>export GLOO_ALGO=Optireduce\nexport GLOO_SOCKET_IFNAME=\"ens17\"  # Your NIC name\nexport GLOO_DPDK_THREADS_OFFSET=11 # OptiReduce cores (will be 11-15 here)\n</code></pre>"},{"location":"getting-started/#2-setup-your-code","title":"2. Setup Your Code","text":"<pre><code>import torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\n# Initialize process group\ndist.init_process_group(backend=\"gloo\")\n\n# Create DDP model\nmodel = DDP(model, bucket_cap_mb=1350)  # This value is required\n</code></pre>"},{"location":"getting-started/#3-run-training","title":"3. Run Training","text":"<p>We provide ready-made training scripts for popular models (VGG19, BERT, BART, RoBERTa, GPT2) in our benchmark repository. See our benchmarking guide for running these scripts.</p>"},{"location":"getting-started/#whats-next","title":"What's Next?","text":""},{"location":"getting-started/#understanding-optireduce","title":"Understanding OptiReduce","text":"<ul> <li>Check Usage Guide for detailed configuration options</li> <li>Read Technical Details to learn about OptiReduce's architecture</li> </ul>"},{"location":"getting-started/#optimizing-performance","title":"Optimizing Performance","text":"<ul> <li>Follow our Benchmarking Guide to evaluate performance</li> <li>Learn how to simulate different network environments</li> <li>Compare OptiReduce with other communication schemes</li> </ul>"},{"location":"getting-started/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the documentation pages linked above</li> <li>Review existing GitHub issues</li> <li>Open a new issue with a minimal example</li> </ol>"},{"location":"getting-started/#contributing","title":"Contributing","text":"<p>We welcome contributions to OptiReduce! Whether it's improving documentation, fixing bugs, optimizing performance, or adding new features, your help is appreciated. Please check our Contributing Guide for guidelines on how to get started.</p>"},{"location":"installation/","title":"Installation Guide","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":""},{"location":"installation/#network-interface-card-nic-requirements","title":"Network Interface Card (NIC) Requirements","text":"<p>OptiReduce works best with Mellanox ConnectX NICs as they support DPDK flow bifurcation. This allows:</p> <ul> <li>Single NIC operation for both PyTorch/Gloo TCP-based communication and OptiReduce DPDK-based communication</li> <li>Efficient packet steering and processing</li> <li>Optimal performance with hardware offloading capabilities</li> </ul> <p>If not using Mellanox ConnectX NICs, you will need:</p> <ol> <li>One NIC for standard TCP-based PyTorch and Gloo communication</li> <li>A separate DPDK-compatible NIC for OptiReduce</li> </ol> <p>Note</p> <p>DPDK v20.11 will be installed automatically as part of OptiReduce.</p> <p>Info</p> <p>While CUDA and cuDNN are supported for GPU training, they are not required for OptiReduce to work. OptiReduce can also be used with CPU-only training.</p>"},{"location":"installation/#installation-options","title":"Installation Options","text":"<p>There are two ways to install OptiReduce and its dependencies:</p>"},{"location":"installation/#option-1-using-ansible-recommended","title":"Option 1: Using Ansible (Recommended)","text":"<p>For automated deployment across multiple nodes:</p> <pre><code>git clone https://github.com/OptiReduce/ansible.git\ncd ansible\nmake optireduce-full\n</code></pre> <p>For detailed instructions on using the Ansible deployment, visit our Ansible documentation.</p>"},{"location":"installation/#option-2-manual-installation","title":"Option 2: Manual Installation","text":"<ol> <li> <p>Install prerequisites:</p> <ul> <li>Mellanox OFED drivers (if using Mellanox NICs)</li> <li>Anaconda</li> <li>CUDA and cuDNN (optional, for GPU training)</li> </ul> </li> <li> <p>Install OptiReduce components: <pre><code># Clone the optireduce setup repository\ngit clone https://github.com/OptiReduce/setup.git\ncd setup\n\n# Install all components\nmake install\n\n# Or install specific components\nmake dpdk        # Install DPDK only\nmake optireduce  # Setup OptiReduce only\nmake hadamard    # Install Hadamard CUDA only\n</code></pre></p> </li> </ol> <p>Tip</p> <p>Use <code>make help</code> to see all available installation options.</p>"},{"location":"installation/#directory-structure","title":"Directory Structure","text":"<pre><code>setup/\n\u251c\u2500\u2500 Makefile          # Build and installation scripts\n\u251c\u2500\u2500 patches/          # Required patches for OptiReduce logic\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>For detailed instructions on using OptiReduce in your distributed training setup, please refer to our usage guide.</p> <p>To evaluate OptiReduce's performance and compare different communication schemes in your environment, please refer to our benchmarking guide.</p>"},{"location":"installation/#additional-documentation","title":"Additional Documentation","text":"<ul> <li>Getting Started</li> <li>Technical Details</li> <li>Performance Benchmarks</li> <li>Usage Instructions</li> </ul>"},{"location":"network-tuning/","title":"Network Tuning Guide","text":"<p>This guide covers network optimization and rate control implementation for OptiReduce, particularly important for networks faster than 10 Gbps.</p>"},{"location":"network-tuning/#rate-control-implementation","title":"Rate Control Implementation","text":""},{"location":"network-tuning/#overview","title":"Overview","text":"<p>The open-source version of OptiReduce requires rate control implementation for networks faster than 10 Gbps to prevent packet loss. This can be implemented in the <code>send_mbufs()</code> function.</p>"},{"location":"network-tuning/#implementation-location","title":"Implementation Location","text":"<p>Find the <code>send_mbufs()</code> function in: <pre><code>src/dpdk/dpdk_transport.cpp\n</code></pre></p>"},{"location":"network-tuning/#simple-rate-control-example","title":"Simple Rate Control Example","text":"<p>Here's a basic token bucket implementation for rate control:</p> <pre><code>class TokenBucket {\nprivate:\n    uint64_t tokens;        // Current tokens\n    uint64_t bucket_size;   // Maximum tokens\n    uint64_t token_rate;    // Tokens per second\n    uint64_t last_update;   // Last update timestamp\n\npublic:\n    TokenBucket(uint64_t rate, uint64_t burst_size) {\n        token_rate = rate;\n        bucket_size = burst_size;\n        tokens = bucket_size;\n        last_update = rte_get_timer_cycles();\n    }\n\n    bool consume(uint64_t bytes) {\n        uint64_t now = rte_get_timer_cycles();\n        uint64_t elapsed = (now - last_update) / rte_get_timer_hz();\n\n        // Add new tokens\n        tokens += elapsed * token_rate;\n        if (tokens &gt; bucket_size) {\n            tokens = bucket_size;\n        }\n\n        // Try to consume tokens\n        if (tokens &gt;= bytes) {\n            tokens -= bytes;\n            last_update = now;\n            return true;\n        }\n        return false;\n    }\n};\n\n// Usage in send_mbufs():\nTokenBucket rate_limiter(1250000000, 1562500); // 10 Gbps, 1ms burst\n\nint send_mbufs(struct rte_mbuf **pkts, uint16_t count) {\n    for (uint16_t i = 0; i &lt; count; i++) {\n        while (!rate_limiter.consume(rte_pktmbuf_pkt_len(pkts[i]))) {\n            rte_delay_us(100);  // Wait if rate limit exceeded\n        }\n        // Existing send logic here\n    }\n    return 0;\n}\n</code></pre>"},{"location":"network-tuning/#alternative-approaches","title":"Alternative Approaches","text":""},{"location":"network-tuning/#simple-sleep-based-control","title":"Simple Sleep-based Control","text":"<pre><code>void rate_control(uint64_t bytes_sent, uint64_t target_rate) {\n    static uint64_t last_time = 0;\n    uint64_t now = rte_get_timer_cycles();\n    uint64_t expected_time = (bytes_sent * rte_get_timer_hz()) / target_rate;\n\n    if (now - last_time &lt; expected_time) {\n        rte_delay_us((expected_time - (now - last_time)) * 1000000 / rte_get_timer_hz());\n    }\n    last_time = now;\n}\n</code></pre>"},{"location":"network-tuning/#window-based-control","title":"Window-based Control","text":"<pre><code>class WindowRateControl {\n    uint64_t window_bytes;\n    uint64_t window_start;\n    uint64_t target_rate;\n\npublic:\n    void check_rate(uint64_t new_bytes) {\n        // Implementation\n    }\n};\n</code></pre>"},{"location":"network-tuning/#network-optimization","title":"Network Optimization","text":""},{"location":"network-tuning/#mtu-configuration","title":"MTU Configuration","text":"<pre><code># Check current MTU\nip link show dev &lt;interface&gt;\n\n# Set jumbo frames\nip link set dev &lt;interface&gt; mtu 9000\n</code></pre>"},{"location":"network-tuning/#dpdk-queue-configuration","title":"DPDK Queue Configuration","text":"<pre><code>// In your DPDK initialization\nstruct rte_eth_conf port_conf = {\n    .rxmode = {\n        .max_rx_pkt_len = RTE_ETHER_MAX_JUMBO_FRAME_LEN,\n    },\n    .txmode = {\n        .mq_mode = ETH_MQ_TX_NONE,\n    },\n};\n</code></pre>"},{"location":"network-tuning/#performance-monitoring","title":"Performance Monitoring","text":"<p>Monitor these metrics to optimize rate control:</p> <pre><code># Check packet drops\nethtool -S &lt;interface&gt; | grep drop\n\n# Monitor throughput\niperf3 -c &lt;target&gt; -p &lt;port&gt;\n\n# Check NIC statistics\ndpdk-procinfo\n</code></pre>"},{"location":"network-tuning/#best-practices","title":"Best Practices","text":""},{"location":"network-tuning/#rate-control-parameters","title":"Rate Control Parameters","text":"<ul> <li>Start with 10 Gbps limit</li> <li>Adjust burst size based on workload</li> <li>Monitor packet loss</li> </ul>"},{"location":"network-tuning/#buffer-management","title":"Buffer Management","text":"<ul> <li>Use appropriate buffer sizes</li> <li>Monitor buffer utilization</li> <li>Adjust based on network conditions</li> </ul>"},{"location":"network-tuning/#performance-tuning","title":"Performance Tuning","text":"<ul> <li>Monitor CPU usage</li> <li>Adjust polling intervals</li> <li>Balance between latency and throughput</li> </ul>"},{"location":"network-tuning/#troubleshooting","title":"Troubleshooting","text":""},{"location":"network-tuning/#common-issues","title":"Common Issues","text":""},{"location":"network-tuning/#high-packet-loss","title":"High Packet Loss","text":"<ul> <li>Reduce transmission rate</li> <li>Increase buffer sizes</li> <li>Check network congestion</li> </ul>"},{"location":"network-tuning/#inconsistent-performance","title":"Inconsistent Performance","text":"<ul> <li>Verify rate control implementation</li> <li>Check system resources</li> <li>Monitor network conditions</li> </ul>"},{"location":"network-tuning/#next-steps","title":"Next Steps","text":"<p>After implementing rate control:</p> <ol> <li>Run benchmarks to verify performance</li> <li>Monitor network metrics</li> <li>Tune parameters as needed</li> </ol>"},{"location":"publications/","title":"Publications","text":""},{"location":"publications/#research-paper","title":"Research Paper","text":"<p>OptiReduce: Resilient and Tail-Optimal AllReduce for Distributed Deep Learning in the Cloud Presented at USENIX NSDI 2025  </p>"},{"location":"publications/#authors","title":"Authors","text":"<ul> <li>Ertza Warraich (Purdue University)</li> <li>Omer Shabtai (NVIDIA)</li> <li>Khalid Manaa (NVIDIA)</li> <li>Shay Vargaftik (VMware Research)</li> <li>Yonatan Piasetzky (NVIDIA)</li> <li>Matty Kadosh (NVIDIA)</li> <li>Lalith Suresh (Feldera)</li> <li>Muhammad Shahbaz (Michigan University)</li> </ul>"},{"location":"publications/#citation","title":"Citation","text":"<p>Please cite this paper when using OptiReduce:</p> <pre><code>@inproceedings{warraich2025optireduce,\n    title={OptiReduce: Resilient and Tail-Optimal AllReduce for Distributed Deep Learning in the Cloud},\n    author={Warraich, Ertza and Shabtai, Omer and Manaa, Khalid and Vargaftik, Shay and Piasetzky, Yonatan and Kadosh, Matty and Suresh, Lalith and Shahbaz, Muhammad},\n    booktitle={22nd USENIX Symposium on Networked Systems Design and Implementation (NSDI 25)},\n    year={2025},\n    publisher={USENIX Association}\n}\n</code></pre>"},{"location":"publications/#technical-documentation","title":"Technical Documentation","text":"<p>We maintain detailed documentation about OptiReduce's:</p> <ul> <li>Getting Started and Usage</li> <li>Architecture and Implementation</li> <li>Performance Analysis</li> </ul>"},{"location":"publications/#contact","title":"Contact","text":"<p>For research-related queries or collaborations:</p> <ul> <li>Email: ewarraic@purdue.edu</li> </ul>"},{"location":"technical-details/","title":"Technical Details","text":"<p>This document describes the technical architecture and implementation details of OptiReduce.</p>"},{"location":"technical-details/#architecture-overview","title":"Architecture Overview","text":"<p>OptiReduce integrates with PyTorch's DistributedDataParallel (DDP) through the Gloo backend, implementing a tail-optimized DPDK-based communication layer.</p>"},{"location":"technical-details/#component-stack","title":"Component Stack","text":"<pre><code>graph TD\n    A[PyTorch DDP] --&gt; B[Gloo Backend]\n    B --&gt; C[OptiReduce Layer]\n    C --&gt; D[DPDK Runtime]\n    D --&gt; E[Network Hardware]</code></pre>"},{"location":"technical-details/#core-components","title":"Core Components","text":""},{"location":"technical-details/#pytorch-integration","title":"PyTorch Integration","text":"<p>OptiReduce seamlessly integrates with PyTorch through:</p> <ul> <li>Standard DDP interface</li> <li>Gloo backend integration</li> <li>Simple activation via <code>GLOO_ALGO=Optireduce</code></li> </ul>"},{"location":"technical-details/#dpdk-communication-layer","title":"DPDK Communication Layer","text":"<p>Our DPDK layer provides:</p> <ul> <li>Zero-copy packet processing</li> <li>Direct NIC access with kernel bypass</li> <li>Multiple dedicated rings:<ul> <li>4 RX rings for receiving</li> <li>1 TX ring for transmission</li> </ul> </li> <li>Optimized ring buffers:<ul> <li>RX ring size: 8192 entries</li> <li>TX ring size: 128 entries</li> <li>Optimized memory pool management</li> </ul> </li> </ul>"},{"location":"technical-details/#memory-management","title":"Memory Management","text":""},{"location":"technical-details/#memory-pools","title":"Memory Pools","text":"<p>OptiReduce implements efficient memory management through:</p> <ul> <li>Optimized mbuf pool management</li> <li>Separate RX and TX pools</li> <li>Cache size of 250 entries</li> <li>MTU-based dynamic buffer sizing</li> </ul>"},{"location":"technical-details/#ring-buffer-organization","title":"Ring Buffer Organization","text":"<p>The ring buffer system features:</p> <ul> <li>Pre-allocated memory pools</li> <li>Zero-copy operations</li> <li>Configurable buffer sizes</li> <li>DPDK-optimized operations</li> </ul>"},{"location":"technical-details/#protocol-design","title":"Protocol Design","text":""},{"location":"technical-details/#packet-structure","title":"Packet Structure","text":"<p>OptiReduce uses a custom packet format:</p> <pre><code>+----------------+-------------+------------+-------------+------------------+\n| Ethernet HDR   | IP HDR      | UDP HDR    | OptiReduce  | Payload          |\n|                |             |            | HDR         |                  |\n+----------------+-------------+------------+-------------+------------------+\n</code></pre> <p>The OptiReduce header contains:</p> <pre><code>struct rte_ult_hdr {\n    uint64_t offset;    // Offset in the buffer\n    uint16_t counter;   // Message counter\n    uint16_t timeout;   // Timeout value\n    uint16_t length;    // Payload length\n    size_t rank;        // Sender rank\n    bool last;          // Last packet indicator\n};\n</code></pre>"},{"location":"technical-details/#performance-optimization","title":"Performance Optimization","text":""},{"location":"technical-details/#communication-patterns","title":"Communication Patterns","text":"<p>OptiReduce optimizes communication through:</p> <ul> <li>High-tail latency environment handling</li> <li>Network variability management</li> <li>Adaptive timeout mechanisms</li> </ul>"},{"location":"technical-details/#resource-requirements","title":"Resource Requirements","text":""},{"location":"technical-details/#cpu-resources","title":"CPU Resources","text":"<ul> <li>4 dedicated cores required for RX processing:<ul> <li>Each core handles a separate RX ring</li> <li>Dedicated threads for receive and reduce operations</li> <li>Two parallel allreduce operations from PyTorch DDP</li> <li>Separate rings prevent gradient mixing between concurrent operations</li> </ul> </li> </ul>"},{"location":"technical-details/#memory-resources","title":"Memory Resources","text":"<ul> <li>Hugepage management</li> <li>Zero-copy operations</li> <li>Direct memory access</li> <li>Optimized buffer handling</li> </ul>"},{"location":"technical-details/#configuration","title":"Configuration","text":""},{"location":"technical-details/#environment-variables","title":"Environment Variables","text":"<pre><code># Core Configuration\nGLOO_ALGO=\"OptiReduce\"                 # Enable OptiReduce\nGLOO_SOCKET_IFNAME=\"ens17\"             # Network interface\nGLOO_DPDK_TIMEOUT=10000                # Operation timeout (ms)\nGLOO_DPDK_THREADS_OFFSET=11            # Core offset for threads\nGLOO_DPDK_SEND_TIMER=true              # Enable sender timeout\nGLOO_DPDK_FILE_PREFIX=\"/path/to/file.log\"  # Log file path\nGLOO_DPDK_CONFIG=\"/path/to/dpdk.cfg\"       # DPDK config file location\n</code></pre>"},{"location":"technical-details/#dpdk-configuration-file","title":"DPDK Configuration File","text":"<p>The configuration file maps IP addresses to MAC addresses:</p> <pre><code>192.168.100.10=AA:BB:CC:DD:EE:FF\n192.168.100.11=AA:BB:CC:DD:EE:00\n</code></pre>"},{"location":"technical-details/#current-limitations","title":"Current Limitations","text":"<ul> <li>Maximum 2 concurrent buckets supported</li> <li>Must configure DDP with a large bucket size: <pre><code>model = DDP(model, bucket_cap_mb=1350)\n</code></pre></li> </ul> <p>Info</p> <p>An experimental branch supporting an arbitrary number of buckets per allreduce call exists at OptiReduce Setup. Feel free to check it out!</p>"},{"location":"technical-details/#network-requirements","title":"Network Requirements","text":"<ul> <li>Open-source version requires rate control for &gt;10 Gbps networks</li> <li>Requires DPDK-compatible network cards</li> <li>Optimized for Mellanox ConnectX NICs</li> </ul>"},{"location":"technical-details/#future-development","title":"Future Development","text":"<p>Planned enhancements include:</p> <ul> <li>Extended bucket support</li> <li>Open-source Timely-based rate control</li> </ul>"},{"location":"technical-details/#learn-more","title":"Learn More","text":"<ul> <li>Read our research paper for in-depth design details</li> <li>Review installation guide for setup instructions</li> <li>Explore benchmarking guide to evaluate performance in your environment</li> </ul>"},{"location":"usage/","title":"Using OptiReduce","text":"<p>This guide explains how to use OptiReduce with PyTorch Distributed Data Parallel (DDP) training. OptiReduce integrates seamlessly with PyTorch's DDP using the Gloo backend.</p>"},{"location":"usage/#prerequisites","title":"Prerequisites","text":"<p>Before using OptiReduce, ensure you have:</p>"},{"location":"usage/#network-setup","title":"Network Setup","text":"<ul> <li>Mellanox ConnectX NIC (recommended)</li> <li>Or two NICs: one for TCP and one DPDK-compatible NIC</li> <li>DPDK v20.11 (installed automatically with OptiReduce)</li> </ul>"},{"location":"usage/#system-configuration","title":"System Configuration","text":"<ul> <li>Hugepages configuration (16GB total)</li> <li>At least 4 dedicated CPU cores for OptiReduce</li> </ul>"},{"location":"usage/#configuration","title":"Configuration","text":""},{"location":"usage/#1-hugepages-setup","title":"1. Hugepages Setup","text":"<p>Configure 16GB of hugepages using one of these methods:</p>"},{"location":"usage/#using-1gb-hugepages-recommended","title":"Using 1GB Hugepages (Recommended)","text":"<pre><code># Add to GRUB_CMDLINE_LINUX in /etc/default/grub\ndefault_hugepagesz=1G hugepagesz=1G hugepages=16\n\n# Update and reboot\nsudo update-grub\nsudo reboot\n</code></pre>"},{"location":"usage/#using-2mb-hugepages","title":"Using 2MB Hugepages","text":"<pre><code># Add to GRUB_CMDLINE_LINUX in /etc/default/grub\ndefault_hugepagesz=2M hugepagesz=2M hugepages=8192\n\n# Update and reboot\nsudo update-grub\nsudo reboot\n</code></pre> <p>Verify configuration: <pre><code>cat /proc/meminfo | grep Huge\n</code></pre></p>"},{"location":"usage/#2-dpdk-configuration","title":"2. DPDK Configuration","text":"<p>Create a DPDK configuration file (<code>dpdk.cfg</code>) mapping IP addresses to MAC addresses for all nodes:</p> <pre><code>192.168.100.10=AA:BB:CC:DD:EE:FF\n192.168.100.11=AA:BB:CC:DD:EE:00\n</code></pre>"},{"location":"usage/#3-environment-variables","title":"3. Environment Variables","text":"<p>Set these required environment variables:</p> <pre><code># Enable OptiReduce\nexport GLOO_ALGO=Optireduce\n\n# Network interface to use\nexport GLOO_SOCKET_IFNAME=\"ens17\"  # Use your DPDK-enabled NIC name\n\n# Path to config file (default: ./dpdk.cfg)\nexport GLOO_DPDK_CONFIG=\"/path/to/dpdk.cfg\"\n\n# Core offset for DPDK threads (requires 4 cores)\nexport GLOO_DPDK_THREADS_OFFSET=11\n\n# Timeout for allreduce operations (milliseconds)\nexport GLOO_DPDK_TIMEOUT=10000\n\n# Enable sender-side timeouts (Optional: off by default)\nexport GLOO_DPDK_SEND_TIMER=true\n</code></pre>"},{"location":"usage/#basic-usage","title":"Basic Usage","text":"<p>Here's how to use OptiReduce with PyTorch DDP:</p> <pre><code>import os\nimport torch\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef setup_optireduce():\n    # Set OptiReduce environment variables\n    os.environ[\"GLOO_ALGO\"] = \"Optireduce\"\n    os.environ[\"GLOO_DPDK_CONFIG\"] = \"/path/to/dpdk.cfg\"\n    os.environ[\"GLOO_SOCKET_IFNAME\"] = \"ens17\"\n\n    # Initialize process group\n    dist.init_process_group(backend=\"gloo\")\n\ndef main():\n    # Setup OptiReduce\n    setup_optireduce()\n\n    # Create model\n    model = YourModel()\n\n    # CRITICAL: Must set bucket_cap_mb=1350\n    # OptiReduce supports only 2 concurrent buckets so set bucket_cap_mb to a large value\n    model = DDP(model, bucket_cap_mb=1350)  # DO NOT CHANGE THIS VALUE\n\n    # Your training loop here\n    ...\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Important</p> <p>The current implementation supports only two concurrent buckets. You must set <code>bucket_cap_mb=1350</code> (or a large value) when creating your DDP model. Failing to do so can lead to crashes.</p>"},{"location":"usage/#running-training-and-performance-evaluation","title":"Running Training and Performance Evaluation","text":"<p>For running training with OptiReduce, we provide ready-made scripts for various models (VGG19, BERT, BART, RoBERTa, GPT2) in our benchmark repository.</p> <p>To evaluate performance:</p> <ol> <li>Follow our benchmarking guide</li> <li>Use provided scripts to simulate different network conditions</li> <li>Compare OptiReduce with other communication schemes</li> </ol>"},{"location":"usage/#next-steps","title":"Next Steps","text":"<ul> <li>See Installation Instructions for setup details</li> <li>Review Technical Details for architecture information</li> <li>Check Benchmarks for performance evaluation guide</li> </ul>"}]}